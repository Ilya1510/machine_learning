{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы научитесь:\n",
    "\n",
    "    работать с градиентным бустингом и подбирать его гиперпараметры\n",
    "    сравнивать разные способы построения композиций\n",
    "    понимать, в каком случае лучше использовать случайный лес, а в каком — градиентный бустинг\n",
    "\n",
    "    использовать метрику log-loss\n",
    "    \n",
    "Введение\n",
    "\n",
    "Построение композиции — важный подход в машинном обучении, который позволяет объединять большое количество слабых алгоритмов в один сильный. Данный подход широко используется на практике в самых разных задачах.\n",
    "\n",
    "На лекциях был рассмотрен метод градиентного бустинга, который последовательно строит композицию алгоритмов, причем каждый следующий алгоритм выбирается так, чтобы исправлять ошибки уже имеющейся композиции. Обычно в качестве базовых алгоритмов используют деревья небольшой глубины, поскольку их достаточно легко строить, и при этом они дают нелинейные разделяющие поверхности.\n",
    "\n",
    "Другой метод построения композиций — случайный лес. В нем, в отличие от градиентного бустинга, отдельные деревья строятся независимо и без каких-либо ограничений на глубину — дерево наращивается до тех пор, пока не покажет наилучшее качество на обучающей выборке.\n",
    "\n",
    "В этом задании мы будем иметь дело с задачей классификации. В качестве функции потерь будем использовать log-loss:\n",
    "    $$L(y, z) = -y log(z) - (1 - y) log(1 - z)$$\n",
    "Здесь через y обозначен истинный ответ, через z — прогноз алгоритма. Данная функция является дифференцируемой, и поэтому подходит для использования в градиентном бустинге. Также можно показать, что при ее использовании итоговый алгоритм будет приближать истинные вероятности классов.\n",
    "Реализация в sklearn\n",
    "\n",
    "В пакете scikit-learn градиентный бустинг реализован в модуле ensemble в виде классов GradientBoostingClassifier и GradientBoostingRegressor. Основные параметры, которые будут интересовать нас: n_estimators, learning_rate. Иногда может быть полезен параметр verbose для отслеживания процесса обучения.\n",
    "\n",
    "Чтобы была возможность оценить качество построенной композиции на каждой итерации, у класса есть метод staged_decision_function. Для заданной выборки он возвращает ответ на каждой итерации.\n",
    "\n",
    "Помимо алгоритмов машинного обучения, в пакете scikit-learn представлено большое число различных инструментов. В этом задании будет предложено воспользоваться функцией train_test_split модуля cross_validation. С помощью нее можно разбивать выборки случайным образом. На вход можно передать несколько выборок (с условием, что они имеют одинаковое количество строк). Пусть, например, имеются данные X и y, где X — это признаковое описание объектов, y — целевое значение. Тогда следующий код будет удобен для разбиения этих данных на обучающее и тестовое множества:\n",
    "\n",
    "    X_train, X_test, y_train, y_test = \n",
    "                         train_test_split(X, y, \n",
    "                                          test_size=0.33, \n",
    "                                          random_state=42)\n",
    "Обратите внимание, что при фиксированном параметре random_state результат разбиения можно воспроизвести.\n",
    "\n",
    "Метрика log-loss реализована в пакете metrics: sklearn.metrics.log_loss. Заметим, что данная метрика предназначена для классификаторов, выдающих оценку принадлежности классу, а не бинарные ответы. И градиентный бустинг, и случайный лес умеют строить такие прогнозы — для этого нужно использовать метод predict_proba:\n",
    "    \n",
    "    pred = clf.predict_proba(X_test)\n",
    "    \n",
    "Метод predict_proba возвращает матрицу, i-й столбец которой содержит оценки принадлежности i-му классу.\n",
    "\n",
    "Для рисования кривых качества на обучении и контроле можно воспользоваться следующим кодом:\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    plt.figure()\n",
    "    plt.plot(test_loss, 'r', linewidth=2)\n",
    "    plt.plot(train_loss, 'g', linewidth=2)\n",
    "    plt.legend(['test', 'train'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Загрузите выборку из файла gbm-data.csv с помощью pandas и преобразуйте ее в массив numpy (параметр values у датафрейма). В первой колонке файла с данными записано, была или нет реакция. Все остальные колонки (d1 - d1776) содержат различные характеристики молекулы, такие как размер, форма и т.д. Разбейте выборку на обучающую и тестовую, используя функцию train_test_split с параметрами test_size = 0.8 и random_state = 241. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3751, 1777)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Activity</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>D3</th>\n",
       "      <th>D4</th>\n",
       "      <th>D5</th>\n",
       "      <th>D6</th>\n",
       "      <th>D7</th>\n",
       "      <th>D8</th>\n",
       "      <th>D9</th>\n",
       "      <th>...</th>\n",
       "      <th>D1767</th>\n",
       "      <th>D1768</th>\n",
       "      <th>D1769</th>\n",
       "      <th>D1770</th>\n",
       "      <th>D1771</th>\n",
       "      <th>D1772</th>\n",
       "      <th>D1773</th>\n",
       "      <th>D1774</th>\n",
       "      <th>D1775</th>\n",
       "      <th>D1776</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>1</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.651023</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.151154</td>\n",
       "      <td>0.766505</td>\n",
       "      <td>0.170876</td>\n",
       "      <td>0.404546</td>\n",
       "      <td>0.787935</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.520564</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.179949</td>\n",
       "      <td>0.768785</td>\n",
       "      <td>0.177341</td>\n",
       "      <td>0.471179</td>\n",
       "      <td>0.872241</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>1</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.765646</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.536954</td>\n",
       "      <td>0.634936</td>\n",
       "      <td>0.342713</td>\n",
       "      <td>0.447162</td>\n",
       "      <td>0.672689</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>0</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.533952</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.347966</td>\n",
       "      <td>0.757971</td>\n",
       "      <td>0.230667</td>\n",
       "      <td>0.272652</td>\n",
       "      <td>0.854116</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 1777 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Activity        D1        D2    D3   D4        D5        D6        D7  \\\n",
       "3747         1  0.133333  0.651023  0.15  0.0  0.151154  0.766505  0.170876   \n",
       "3748         0  0.200000  0.520564  0.00  0.0  0.179949  0.768785  0.177341   \n",
       "3749         1  0.100000  0.765646  0.00  0.0  0.536954  0.634936  0.342713   \n",
       "3750         0  0.133333  0.533952  0.00  0.0  0.347966  0.757971  0.230667   \n",
       "\n",
       "            D8        D9  ...    D1767  D1768  D1769  D1770  D1771  D1772  \\\n",
       "3747  0.404546  0.787935  ...        0      0      1      0      1      0   \n",
       "3748  0.471179  0.872241  ...        0      0      0      0      0      0   \n",
       "3749  0.447162  0.672689  ...        0      0      0      0      0      0   \n",
       "3750  0.272652  0.854116  ...        0      0      0      0      0      0   \n",
       "\n",
       "      D1773  D1774  D1775  D1776  \n",
       "3747      1      0      0      0  \n",
       "3748      0      0      0      0  \n",
       "3749      0      0      0      0  \n",
       "3750      0      0      0      0  \n",
       "\n",
       "[4 rows x 1777 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('gbm-data.csv')\n",
    "print(data.shape)\n",
    "data.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (3751, 1777)\n"
     ]
    }
   ],
   "source": [
    "data_arr = data.values\n",
    "print(type(data_arr), data_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3751, 1776) (3751,)\n"
     ]
    }
   ],
   "source": [
    "X = data_arr[:, 1:]\n",
    "y = data_arr[:, 0]\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1776) (3000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=241)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Обучите GradientBoostingClassifier с параметрами n_estimators=250, verbose=True, random_state=241 и для каждого значения learning_rate из списка [1, 0.5, 0.3, 0.2, 0.1] проделайте следующее:\n",
    "\n",
    "    Используйте метод staged_decision_function для предсказания качества на обучающей и тестовой выборке на каждой итерации.\n",
    "    Преобразуйте полученное предсказание с помощью сигмоидной функции по формуле 1 / (1 + e^{−y_pred}), где y_pred — предсказанное значение.\n",
    "    Вычислите и постройте график значений log-loss (которую можно посчитать с помощью функции sklearn.metrics.log_loss) на обучающей и тестовой выборках, а также найдите минимальное значение метрики и номер итерации, на которой оно достигается."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "import numpy as np\n",
    "\n",
    "def analize_diff_rate(iterable):\n",
    "    min_test_loss = {}\n",
    "    for rate in iterable:\n",
    "        gbc = GradientBoostingClassifier(n_estimators=1000, verbose=True, learning_rate=rate, random_state=241)\n",
    "        gbc.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred_train = [1 / (1 + np.exp(-i)) for i in gbc.staged_decision_function(X_train)] #i is np.array\n",
    "        y_pred_test = [1 / (1 + np.exp(-i)) for i in gbc.staged_decision_function(X_test)]\n",
    "        scores_train = {}\n",
    "        scores_test = {}\n",
    "        for i in range(len(y_pred_train)):\n",
    "            scores_train[i] = log_loss(y_train, y_pred_train[i])\n",
    "            scores_test[i] = log_loss(y_test, y_pred_test[i])    \n",
    "        \n",
    "        plt.title('log loss with rate: ' + str(rate))\n",
    "        plt.plot(list(scores_train.keys()), list(scores_train.values()), 'b', linewidth=2)\n",
    "        plt.plot(list(scores_test.keys()), list(scores_test.values()), 'r', linewidth=2)\n",
    "        plt.legend(['train', 'test'])\n",
    "        plt.show()\n",
    "        \n",
    "        test_loss = sorted(scores_test.items(), key=lambda x: x[1])\n",
    "        min_test_loss[rate] = (test_loss[0][0], test_loss[0][1])\n",
    "    return min_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1           1.2098            5.52m\n",
      "         2           1.1164            5.50m\n",
      "         3           1.0618            5.53m\n",
      "         4           1.0240            5.53m\n",
      "         5           0.9976            5.53m\n",
      "         6           0.9739            5.45m\n",
      "         7           0.9548            5.27m\n",
      "         8           0.9378            5.21m\n",
      "         9           0.9239            5.16m\n",
      "        10           0.9117            5.12m\n",
      "        20           0.8178            4.91m\n",
      "        30           0.7517            4.60m\n",
      "        40           0.6877            4.44m\n",
      "        50           0.6416            4.31m\n",
      "        60           0.5860            4.41m\n",
      "        70           0.5500            4.33m\n",
      "        80           0.5086            4.37m\n",
      "        90           0.4820            4.38m\n",
      "       100           0.4507            4.29m\n",
      "       200           0.2466            3.71m\n",
      "       300           0.1521            3.24m\n",
      "       400           0.0965            2.80m\n",
      "       500           0.0636            2.33m\n",
      "       600           0.0408            1.88m\n",
      "       700           0.0266            1.40m\n",
      "       800           0.0174           56.26s\n",
      "       900           0.0116           28.14s\n",
      "      1000           0.0077            0.00s\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXBL6f8+uSn8lXIgpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5a1fe2c0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "min_loss = analize_diff_rate([0.3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Как можно охарактеризовать график качества на тестовой выборке, начиная с некоторой итерации: переобучение (overfitting) или недообучение (underfitting)? В ответе укажите одно из слов overfitting либо underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overfitting"
     ]
    }
   ],
   "source": [
    "with open('grad_boost_ans1.txt', 'w') as f:\n",
    "    f.write('overfitting')\n",
    "!cat grad_boost_ans1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведите минимальное значение log-loss на тестовой выборке и номер итерации, на котором оно достигается, при learning_rate = 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53 36"
     ]
    }
   ],
   "source": [
    "with open('grad_boost_ans2.txt', 'w') as f:\n",
    "    f.write(' '.join([str(round(x, 2)) for x in [min_loss[0.2][1], min_loss[0.2][0]]]))\n",
    "!cat grad_boost_ans2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0.1: (51, 0.52692018722758438),\n",
       " 0.2: (36, 0.53145079631906378),\n",
       " 0.3: (10, 0.54231411100245541),\n",
       " 0.5: (6, 0.55820255231642613),\n",
       " 1: (0, 0.58229425942784763)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этих же данных обучите RandomForestClassifier с количеством деревьев, равным количеству итераций, на котором достигается наилучшее качество у градиентного бустинга из предыдущего пункта, c random_state=241 и остальными параметрами по умолчанию. Какое значение log-loss на тесте получается у этого случайного леса? (Не забывайте, что предсказания нужно получать с помощью функции predict_proba. В данном случае брать сигмоиду от оценки вероятности класса не нужно)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=720, n_jobs=-1, oob_score=False, random_state=241,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=min_loss[0.2][0] * 20, random_state=241, n_jobs=-1)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_test_rfc = [i[1] for i in rfc.predict_proba(X_test)]\n",
    "loss_rfc = log_loss(y_test, y_pred_test_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52309343555858268"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.54"
     ]
    }
   ],
   "source": [
    "with open('grad_boost_ans3.txt', 'w') as f:\n",
    "    f.write(str(round(loss_rfc, 2)))\n",
    "!cat grad_boost_ans3.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
